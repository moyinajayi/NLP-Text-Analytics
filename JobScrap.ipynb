{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "opts = Options()\n",
    "opts.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\")\n",
    "#opts.add_argument('headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=opts)\n",
    "time.sleep(5)\n",
    "\n",
    "#start driver on main url, \n",
    "#url_main = 'https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=0&page=1&isHiringSurge=0' #main url\n",
    "url_main = 'https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=0&page=1&locId=3&locType=N&locName=Canada&filterType=RATING_OVERALL'\n",
    "\n",
    "driver.get(url_main)\n",
    "#time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unsuccessful_links = [] ##UPDATE## this line to create unique list for this scrape attempt\n",
    "companies = [] ##UPDATE## this line to create unique list for this scrape attempt\n",
    "\n",
    "\n",
    "def scraping_pages(num_pages):\n",
    "    #Creating 'n' urls with url_roots to scrape\n",
    "    url_root = 'https://www.glassdoor.com/Explore/browse-companies.htm?overall_rating_low=0&page=1&locId=3&locType=N&locName=Canada&filterType=RATING_OVERALL' #root url\n",
    "    nums = [x+1 for x in range(num_pages)] ##UPDATE## x + __ according to where last scrape attempt left off\n",
    "    url_mains = list(map(lambda n: url_root + str(n), nums)) #adding 'n' number to call url_root \n",
    "    time.sleep(10) #give page plenty of time to load (page 1 loads first, then specified 'n' page)\n",
    "    \n",
    "    for u in url_mains:\n",
    "        driver.get(u)\n",
    "        time.sleep(10)\n",
    "        \n",
    "    #looking for 'Overview' links from each main search page\n",
    "        elems = driver.find_elements_by_tag_name('a') #find links on an individual search page tagged with the 'a' tag\n",
    "        company_links = []\n",
    "        for elem in elems:\n",
    "            company_link = elem.get_attribute('href') #returns every item with 'href' attribute (these are the links for each company)\n",
    "            if 'Overview' in company_link:\n",
    "                company_links.append(company_link) #each company's 'Overview' link added to company_link list  \n",
    "    \n",
    "    #iterating through each company's \"Overview\" url\n",
    "        for url in company_links:\n",
    "            try: #fail safe for inevitable errors\n",
    "                driver.get(url)\n",
    "                time.sleep(5)\n",
    "\n",
    "                name = 'blackiemaj@gmail.com' # <---- ENTER GLASSDOOR CREDENTIALS HERE\n",
    "                pw = 'Monday123@'\n",
    "                \n",
    "                try: #login                    \n",
    "                    username = driver.find_element_by_id(\"userEmail\")\n",
    "                    password = driver.find_element_by_id(\"userPassword\")\n",
    "                    submit = driver.find_element_by_xpath('//*[@id=\"InlineLoginModule\"]/div/div[2]/div/div[1]/div[3]/form/div[3]/div[1]/button')\n",
    "                    username.send_keys(name)\n",
    "                    password.send_keys(pw)\n",
    "                    submit.click()\n",
    "                    time.sleep(4) #let page load\n",
    "                except: #no login required\n",
    "                    time.sleep(2)\n",
    "                    pass\n",
    "\n",
    "##---------------------------------- Gathering Variables - Main Page ---------------------------------##                                \n",
    "                name = driver.find_element_by_xpath('//*[@id=\"EmpHeroAndEmpInfo\"]/div[3]/div[2]').text\n",
    "                size = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[1]/ul/li[3]/div').text\n",
    "                headquarters = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[1]/ul/li[2]/div').text\n",
    "                industry = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[1]/ul/li[6]/div').text\n",
    "                try:\n",
    "                    num_reviews = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[3]/div[3]/a').text\n",
    "                except: \n",
    "                    num_reviews = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[4]/div[3]/a').text        \n",
    "\n",
    "            #Gather Description - handling \"Read More\" button\n",
    "                try:\n",
    "                    read_more = driver.find_element_by_class_name('css-1tgo67c.e16x8fv00') #button class \n",
    "                    read_more.click()\n",
    "                    time.sleep(2)\n",
    "                    description = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[1]/div[1]/span').text\n",
    "                except:\n",
    "                    description = \"N/A\"\n",
    "\n",
    "            #Gather Mission - handling \"Read More\" button    \n",
    "                try:\n",
    "                    read_more = driver.find_element_by_class_name('css-1tgo67c.e16x8fv00') #button class\n",
    "                    read_more.click()\n",
    "                    time.sleep(2)\n",
    "                    mission = driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[1]/div[2]').text\n",
    "                except:\n",
    "                    mission = \"N/A\"\n",
    "\n",
    "##-------------------------------- Gathering Variables - Ratings Pop-up --------------------------------##    \n",
    "            #Webpage layout 1\n",
    "                try: \n",
    "                    driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[3]/div[1]/div[2]').click()\n",
    "                    time.sleep(5) #let page load\n",
    "\n",
    "                    rating_overall = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[1]/div/div[3]').text\n",
    "                    rating_DI = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[3]/div/div[3]').text\n",
    "                    rating_CV = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[2]/div/div[3]').text\n",
    "                    rating_WL = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[4]/div/div[3]').text\n",
    "                    rating_SM = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[5]/div/div[3]').text\n",
    "                    rating_CB = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[6]/div/div[3]').text\n",
    "                    rating_CO = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[7]/div/div[3]').text\n",
    "\n",
    "                    time.sleep(np.random.choice([x/10 for x in range(7,22)])) #some time to rest \n",
    "            #Webpage layout 2\n",
    "                except: \n",
    "                    driver.get(url) #recalling url\n",
    "                    driver.find_element_by_xpath('//*[@id=\"EIOverviewContainer\"]/div/div[4]/div[1]/div[2]').click()\n",
    "                    time.sleep(5) #let page load\n",
    "                    \n",
    "                    rating_overall = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[1]/div/div[3]').text\n",
    "                    rating_DI = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[3]/div/div[3]').text\n",
    "                    rating_CV = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[2]/div/div[3]').text\n",
    "                    rating_WL = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[4]/div/div[3]').text\n",
    "                    rating_SM = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[5]/div/div[3]').text\n",
    "                    rating_CB = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[6]/div/div[3]').text\n",
    "                    rating_CO = driver.find_element_by_xpath('//*[@id=\"reviewDetailsModal\"]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[7]/div/div[3]').text\n",
    "\n",
    "                    time.sleep(np.random.choice([x/10 for x in range(7,22)])) #some time to rest \n",
    "                                        \n",
    "##---------------------------------------- Creating a Dictionary ----------------------------------------##\n",
    "                companies.append({ ##UPDATE## this line to create unique dictionary for this scrape attempt\n",
    "                    \"NAME\" : name,\n",
    "                    \"SIZE\" : size,\n",
    "                    \"LOCATION_HQ\" : headquarters,\n",
    "                    \"INDUSTRY\" : industry,\n",
    "                    \"RATING_OVERALL\" : rating_overall,\n",
    "                    \"RATING_DI\" : rating_DI,\n",
    "                    \"RATING_CV\" : rating_CV,\n",
    "                    \"RATING_WL\" : rating_WL,\n",
    "                    \"RATING_SM\" : rating_SM,\n",
    "                    \"RATING_CB\" : rating_CB,\n",
    "                    \"RATING_CO\" : rating_CO,\n",
    "                    \"NUM_REVIEWS\" : num_reviews,\n",
    "                    \"DESCRIPTION\" : description,\n",
    "                    \"MISSION\" : mission\n",
    "                                 })\n",
    "\n",
    "            except: #fail safe for inevitable errors\n",
    "                unsuccessful_links.append(url) #adding unsuccessful urls to a list ##UPDATE## unsuccessful_links list\n",
    "                print('ERROR: ', url)\n",
    "                time.sleep(10)\n",
    "                \n",
    "        print(f'Finished scraping {len(companies)} companies') ##UPDATE## companies list name\n",
    "        df = pd.DataFrame(companies)                           ##UPDATE## df number and companies list name\n",
    "    return df                                                  ##UPDATE## df number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_tag_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scraping_pages(\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mscraping_pages\u001b[1;34m(num_pages)\u001b[0m\n\u001b[0;32m     14\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#looking for 'Overview' links from each main search page\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     elems \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements_by_tag_name(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#find links on an individual search page tagged with the 'a' tag\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     company_links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m elems:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_tag_name'"
     ]
    }
   ],
   "source": [
    "scraping_pages(100) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
